# Quantization Benchmarks for Local LLMs

**Repo:** `llm-quantization-benchmarks` â€¢ **Last Updated:** 2025-08-19

## ğŸ¯ Goal

Compare FP16 vs. INT8/INT4 (GGUF/GPTQ/AWQ) on latency, memory, and quality across hardware.

## ğŸ§± Tech Stack

Python, llama.cpp, AutoGPTQ, AWQ, GGUF

## ğŸ”— Upstream / Tools Used

llama.cpp, auto-gptq, awq, gguf

## âœ… Success Metrics

- Tokens/sec (prompt + generation)
- Peak memory usage
- Accuracy on small QA set / MT-Bench-lite score (proxy)

## ğŸš€ Quickstart

```bash
# 1) Create and activate env
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate

# 2) Install
pip install -r requirements.txt

# 3) Run demo
python demo.py
```

## ğŸ“Š Evaluation

- Scripts in `eval/` reproduce metrics.
- Results saved to `results/` as CSV/JSON, summarized in README tables.

## ğŸ§ª Tests

```bash
pytest -q
```

## ğŸ“¦ Structure

```text
llm-quantization-benchmarks/
  â”œâ”€ src/
  â”œâ”€ demo.py
  â”œâ”€ eval/
  â”œâ”€ results/
  â”œâ”€ tests/
  â”œâ”€ requirements.txt
  â””â”€ README.md
```

## ğŸ“¸ Demos

- Scripts to run batch benches
- Markdown tables & plots
-

## ğŸ—ºï¸ Roadmap

- [ ] Define baseline & target metrics
- [ ] Implement MVP
- [ ] Add CI checks
- [ ] Document limitations & next steps

## âš–ï¸ License

MIT (adjust as needed). Respect upstream licenses.
